 - 2장 - 
p.57 선형

XOR = NAND  ┬ AND
            OR    ┘  

 - 3장 - 

p.64 신경망 ☆
 입력층 > 출력층 > 은닉층

은닉층 : 은닉층의 뉴런은 사람 눈에는 보이지 않는다.

b (편향) : 뉴런이 얼마나 쉽게 활성화 되느냐를 제어
w1, w2 (가중치) : 각 신호의 영향력을 제어


p.66 activation function (활성화 함수)
 - 입력 신호의 총합을 출력 신호로 변환하는 함수
 - 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할
 - ex) h(x)

계단함수 : default = x > 0 = 1
                   else = 0

p.68 자연상수(자연로그의 밑) e 
 - 2.7182의 값을 갖는 실수


p.76 ReLu(Rectified Linear Unit) 함수
 - 입력이 0을 넘으면 그 입력을 그대로 출력, 0 이하면 0을 출력
 - h(x) = x ( x > 0)
         = 0 ( x<= 0)

p.78 Numpy 행렬
 - [[1 2]   (행)
    [3 4]
    [5 6]
   (열)
 - 배열의 가로방향을 Row(행), 세로 방향을 Column(열)

p.79 Numpy 행렬의 곱
 - 행렬은 모든게 행·렬 (Numpy.shape, 행렬 곱셈의 결과값의 행렬의 배열순서)

 - shape (행렬의 형상)을 주의
 -   A       B    =    C
  3 * 2   2 * 4      3 * 4
   A의 1번째 열 수(2)와 B의 0번째 행 수(2)가 같다
 - A의 1번째 차원의 원소 수(열 수)와 행렬 B의 0번째 차원의 원소 수(행 수)가 같아야 한다.
 - 행렬 C의 형상은 행렬 A의 행 수와 B의 열 수가 된다.

p.82 신경망에서의 행렬 곱

p.83 신경망에서의 표기법 ☆☆
 - w (1)
      1 2 
 - (1) : 1층의 가중치
 - 1 : 다음 층의 1번째 뉴런
 - 2 : 앞 층의 2번째 뉴런
 - 표기법에서 앞층과 다음층의 순서는 다른경우가 존재함.
 



p.90 출력층 설계
 - Classification (분류) : 데이터가 어느 Class에 속하는지 ex) 사진 속 인물의 성별을 분류
  - 소프트 맥스 함수 사용
 - Regression (회귀) : 입력 데이터에서 (연속적인) 수치를 예측 ex) 사진 속 인물의 몸무게(59.2kg?)를 예측
  - 항등 함수 사용


p.91 출력층 함수
 - Identity Function (항등함수) : 입력을 그대로 출력
 - Softmax Function (소프트맥스 함수) : 
  - exp(x) : e^x를 뜻하는 Exponential Function (지수 함수) [e : 자연상수]
  - n : 출력층의 뉴런 수,
  - y_k : k번째 출력

