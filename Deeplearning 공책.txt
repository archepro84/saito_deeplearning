 - 2장 - 
p.57 선형

XOR = NAND  ┬ AND
            OR    ┘  

 - 3장 - 

p.64 신경망 ☆
 입력층 > 출력층 > 은닉층

은닉층 : 은닉층의 뉴런은 사람 눈에는 보이지 않는다.

b (편향) : 뉴런이 얼마나 쉽게 활성화 되느냐를 제어
w1, w2 (가중치) : 각 신호의 영향력을 제어


p.66 activation function (활성화 함수)
 - 입력 신호의 총합을 출력 신호로 변환하는 함수
 - 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할
 - ex) h(x)

계단함수 : default = x > 0 = 1
                   else = 0

p.68 자연상수(자연로그의 밑) e 
 - 2.7182의 값을 갖는 실수


p.76 ReLu(Rectified Linear Unit) 함수
 - 입력이 0을 넘으면 그 입력을 그대로 출력, 0 이하면 0을 출력
 - h(x) = x ( x > 0)
         = 0 ( x<= 0)

p.78 Numpy 행렬
 - [[1 2]   (행)
    [3 4]
    [5 6]
   (열)
 - 배열의 가로방향을 Row(행), 세로 방향을 Column(열)

p.79 Numpy 행렬의 곱
 - 행렬은 모든게 행·렬 (Numpy.shape, 행렬 곱셈의 결과값의 행렬의 배열순서)

 - shape (행렬의 형상)을 주의
 -   A       B    =    C
  3 * 2   2 * 4      3 * 4
   A의 1번째 열 수(2)와 B의 0번째 행 수(2)가 같다
 - A의 1번째 차원의 원소 수(열 수)와 행렬 B의 0번째 차원의 원소 수(행 수)가 같아야 한다.
 - 행렬 C의 형상은 행렬 A의 행 수와 B의 열 수가 된다.

p.82 신경망에서의 행렬 곱

p.83 신경망에서의 표기법 ☆☆
 - w (1)
      1 2 
 - (1) : 1층의 가중치
 - 1 : 다음 층의 1번째 뉴런
 - 2 : 앞 층의 2번째 뉴런
 - 표기법에서 앞층과 다음층의 순서는 다른경우가 존재함.
 



p.90 출력층 설계
 - Classification (분류) : 데이터가 어느 Class에 속하는지 ex) 사진 속 인물의 성별을 분류
  - 소프트 맥스 함수 사용
 - Regression (회귀) : 입력 데이터에서 (연속적인) 수치를 예측 ex) 사진 속 인물의 몸무게(59.2kg?)를 예측
  - 항등 함수 사용


p.91 출력층 함수 ☆
 - Identity Function (항등함수) : 입력을 그대로 출력
 - Softmax Function (소프트맥스 함수) : 
  - exp(x) : e^x를 뜻하는 Exponential Function (지수 함수) [e : 자연상수]
  - n : 출력층의 뉴런 수,
  - y_k : k번째 출력
  - a_k : 소프트맥스 함수의 분자 = 입력신호 a_k의 지수 함수
  - 분모 : [모든] 입력 신호의 지수 함수의 합 


p.93 소프트맥스 함수 전개과정 ☆
 - 소프트맥스의 지수함수를 게산할 때 어떤 정수를 더해도 (혹은 빼도) 결과는 바뀌지 않는다.
 - 오버플로우를 막을 목적으로는 입력 신호 중 최댓값을 이용하는 것이 일반적.


p.94 소프트맥스 함수의 특징 ☆
 - 소프트맥스 함수의 출력은 0 ~ 1.0사이의 실수.
 - 소프트맥스 함수 출력의 총합은 1
 - 소프트맥스 함수의 출력을 '확률'로 해석
 - 소프트맥스 함수를 이용함으로써 문제를 확률적(통계적)으로 대응할 수 있게 된다.
 - 지수함수 y = exp(x)가 단조 증가 함수이므로 연산을 거치고 난 이후에도 원소들 사이의 대소 관계는 동일하다.
 - 기계학습은 학습과 inlerence (추론)의 두단계를 거친다.
 - 추론단계에서는 출력층이 소프트맥스 함수를 생략하는게 일반적
 - 신경망을 학습시킬 때는 출력층에서 소프트맥스 함수를 사용


p.96 손글씨 숫자 인식
 - forward propagation (순전파) : 이미 학습된 매개변수를 사용하여 추론 과정만 구현
 - pickle : 프로그램 실행 중에 특정 객체를 파일로 저장하는 기능.
  - 저장해둔 pickle 파일을 로드하면 실행 다잇의 객체를 즉시 복원할 수 있음

 - normalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.
    one_hot_label : 
        one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.
        one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.
    flatten : 입력 이미지를 1차원 배열로 만들지를 정한다. 
 - normalization (정규화) : 데이터를 특정 범위로 변환하는 처리
   ex) 0 ~ 255 > 0 ~ 1.0
 - pre processing (전처리) : 신경망의 입력 데이터에 특정 변환을 가하는 것
 - whitening (백색화) : 전체 데이터를 균일하게 분포시키는 전처리 작업 

p.102 배치처리
 - Batch (배치) : 하나로 묶은 입력 데이터



p.106 3장 최종 정리 ☆
 - 신경망에서는 활성화 함수로 시그모이드 함수와 ReLU 함수 같은 매끄럽게 변화하는 함수를 이용한다.
 - 넘파이의 다차원 배열을 잘 사용하면 신경망을 효율적으로 구현할 수 있다.
 - 기계학습 문제는 크게 회귀와 분류로 나눌 수 있다.
 - 출력층의 활성화 함수로는 회귀에서는 주로 항등 함수를, 분류에서는 주로 소프트맥스 함수를 이용한다.
 - 분류에서는 출력층의 뉴런 수를 분류하려는 클래스 수와 같게 설정한다.
 - 입력 데이터를 묶은 것을 배치, 추론 처리를 이 배치 단위로 진행하면 결과를 훨씬 빠르게 얻을 수 있다.


p.107 신경망 학습
 - 학습 : 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것
 - 딥러닝 = end-to-end machine learning (종단간 기계학습) : 데이터(입력)에서 목표한 결과(출력)를 사람의 개입 없이 얻는다.

p.111 Loss Funcion (손실 함수)
 - 신경망 성능의 '나쁨'을 나타내는 지표
 - 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 '못'하느냐를 나타낸다
 - 일반적으로 오차제곱합과 교체 엔트로피 오차를 사용

p.112 Sum of squares for error, SSE (오차제곱합)
 - y_k : 신경망의 출력 (신경망이 추정한 값)
 - t_k : 정답 레이블
 - k : 데이터의 차원수
 - one-hot encoding (원-핫 인코딩) : 한 원소만 1로 하고 그 외는 0으로 나타내는 표기법


p.113 Cross Entropy Error, CEE (교차 엔트로피 오차)
 - log : 밑이 e인 자연로그 (log_e)
 - y_k : 신경망의 출력
 - t_k : 정답 레이블
 - 정답에 해당하는 출력이 커질수록 0에 다가가다가 그 출력이 1일 때 -0이 된다.
 - 반대로 정답일 때의 출력이 작아질수록 오차는 커진다.
 - 즉, 결과(오차 값)가 더 작은 첫 번째 추정이 정답일 가능성이 높다.

p.115 Minibatch (미니배치)
 - Minibatch (미니배치) : 데이터 일부를 추려 전체의 '근사치'로 이용하는것
 - 미니배치 학습 : 60,000장의 훈련 데이터 중에서 100장을 무작위로 뽑아 그 100장만을 사용하여 학습하는것

p.118 (배치용) 교차 엔트로피 오차 구현하기
 - 원-핫 인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0, 그 계산은 무시해도 좋다.
 - 즉 t * np.log(y) >>> np.log(y[np.arange(batch_size),t])로 구현한다.


 - 신경망을 학습할 때 정확도를 지표로 삼아서는 안된다.
   정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다. ☆

 - 정확도는 매개변수의 미소한 변화에는 거의 반응을 보이지 않고, 반응이 있더라고 그 값이 불연속적으로 변화함.
 - 시그모이드 함수의 미분은 어느장소라도 0이 되지는 않는다.
 - 기울기가 0이되지 않는 덕분에 신경망이 올바르게 학습할 수 있다.

p.122 미분
 - Numerical differentiation (수치 미분) : 아주 작은 차분으로 미분하는 것
 - Analytic (해석적) : 수식을 전개해 미분하는 것, 오차를 포함하지 않는 '진정한 미분'값을 구해준다.
   - 수치해석학은 "해석학 문제에서 수치적인 근삿값을 구하는 알고리즘을 연구하는 학문"
 - 중심차분 or 중앙차분 : x를 중심으로 그 전후의 차분을 계산
 - 전방 차분 : ( x + h )와 x의 차분
 - 파이썬 코드에서 rounding error (반올림 오차)를 발생하지 않기 위해 미세한 값 h를 10^-4을 사용한다.

